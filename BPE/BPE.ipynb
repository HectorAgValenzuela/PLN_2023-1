{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de BYTE-PAIR ENCODING\n",
    "Integrantes:\n",
    "*   Aguilar Valenzuela Luis Hector\n",
    "*   Camargo Loaiza Julio Andres\n",
    "*   Minjares Neriz Victor Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Learning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens(vocab)\n",
    "Funcion que recibe un diccionario de palabras, despues transforma el diccionario de palabras a uno de letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(vocab):\n",
    "\n",
    "    # Declara un diccionario vacío\n",
    "    tokens = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario tokens con cada letra y su respectiva frecuencia\n",
    "        for token in word_tokens:\n",
    "            # Guarda la palabra en el diccionario y le suma la frecuencia de la palabra.\n",
    "            tokens[token] += freq\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair, v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas parejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minicorpus a usar :\n",
    "# https://drive.google.com/file/d/17h_rLrWL2xg3jD0U1CCseeaAd6t17yc0/view?usp=share_linket \n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 53, ',': 14, '</w>': 139, 'j': 2, 'u': 18, 'a': 57, 'n': 39, 'g': 7, 'l': 36, 'd': 44, 'e': 91, 'r': 35, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 't': 23, 'ñ': 3, 'q': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 30\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# En este bloque de código se muestran un diccionario de todas las palabras en el vocabulario.\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('e', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',': 14, '</w>': 91, 'j': 2, 'u': 18, 'a': 57, 'n': 39, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 67, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 't': 23, 'ñ': 3, 'q': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 32\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('a', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',': 14, '</w>': 72, 'j': 2, 'u': 18, 'a': 38, 'n': 39, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 67, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 't': 23, 'ñ': 3, 'q': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 33\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('e', 'n')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',': 14, '</w>': 72, 'j': 2, 'u': 18, 'a': 38, 'n': 22, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 50, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 34\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: (',', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 58, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 50, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 34\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: ('l', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 45, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 50, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'l</w>': 13, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 35\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('e', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 45, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'e': 38, 'l</w>': 13, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 36\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('y', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'e': 38, 'l</w>': 13, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 37\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('d', 'e')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 35, 'e</w>': 24, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 38\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('d', 'e</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 23, 'o</w>': 24, 'de</w>': 8, 'd': 27, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 39\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('l', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 27, 'r': 35, 'es': 12, 'c': 25, 'i': 34, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 40\n",
      "==========\n",
      "Iter: 10\n",
      "Best pair: ('d', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 35, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 41\n",
      "==========\n",
      "Iter: 11\n",
      "Best pair: ('a', 'r')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 31, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 42\n",
      "==========\n",
      "Iter: 12\n",
      "Best pair: ('o', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 22, ',</w>': 14, 'j': 2, 'u': 18, 'a': 31, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 19, 'ñ': 3, 'os': 7, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 43\n",
      "==========\n",
      "Iter: 13\n",
      "Best pair: ('q', 'u')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 22, ',</w>': 14, 'j': 2, 'u': 11, 'a': 31, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 19, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 43\n",
      "==========\n",
      "Iter: 14\n",
      "Best pair: ('o', 'n')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 15, ',</w>': 14, 'j': 2, 'u': 11, 'a': 31, 'n': 15, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 19, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'on': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 44\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>\n",
    "                # NOTA : Hacemos todo minusculas para reducir el tiempo de ejecucion\n",
    "                # y el numero de tokens\n",
    "                # NOTA2 : list() aqui puede ser opcional, tal vez\n",
    "                vocab[' '.join(list(word.lower())) + ' </w>'] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair,v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas parejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens_from_vocab(vocab)\n",
    "Funcion que toma como entrada un diccionario con las frecuencias(vocab) y te regresa dos diccionarios. tokens_frequencies es un diccionario de letras/simbolos con sus frecuencias de aparicion en el vocab. \n",
    "vocab_tokenization es un diccionario con las palabras del vocab con su respectiva tokenizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    # Creamos diccionarios vacios\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separamos la palabra en sus letras\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario con las letras/simbolos y sus frecuencias\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        # En este otro lo llenamos con las palabras y su tokenizacion\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  measure_token_length(token)\n",
    "Función que mide la longitud de un token de texto, incluyendo o no un espacio en blanco dependiendo de si el token termina con \"\\</w>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token):\n",
    "    # Verificamos si el token es el final de una palabra, esto es\n",
    "    # si los ultimos 4 caracteres son </w>\n",
    "    if token[-4:] == '</w>':\n",
    "        # Si lo es, entonces devolvemos la longitud sin esos 4 \n",
    "        # caracteres y le agregamos un espacio en blanco\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        # De lo contrario, devolvemos la longitud tal cual\n",
    "        return len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_word(string, sorted_tokens, unknown_token='\\</u>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que tokeniza una cadena de texto dada, string, en base a una lista de tokens ordenados, sorted_tokens. \\</u> es la expresion regular para token desconocido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    #  Si la cadena a tokenizar es un espacio en blanco o una palabra \n",
    "    # fuera del vocabulario regresamos una lista vacia o con la palabra\n",
    "    # desconocida\n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    # Arreglo donde guardaremos los tokens\n",
    "    string_tokens = []\n",
    "    # Iteramos por cada letra/simbolo del vocabulario\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        # Agarramos un elemento del vocabulario\n",
    "        token = sorted_tokens[i]\n",
    "        # Evitamos que se tomen caracteres del token como\n",
    "        # caracteres especiales de expresiones regulares\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        # Se busca todas las coincidencias que tiene el token, token_reg, en la cadena de \n",
    "        # texto, string, y se almacenan sus posiciones inicial y final\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        # Si no hay coincidencias pasamos al siguiente token    \n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "\n",
    "        # Agarramos las posiciones finales de todas las coincidencias e inicializamos\n",
    "        # el comienzo en 0, esto es el principio del string a tokenizar\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "        substring_start_position = 0\n",
    "        # Iteramos sobre todas las posiciones finales de las coincidencias\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # Dividiremos la cadena en una subcadena\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            # Llamamos recursivamente para dividir la subcadena en tokens con una \n",
    "            # nueva lista de tokens que contiene todos los tokens en sorted_tokens excepto los \n",
    "            # que ya han sido utilizados\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            # Agregamos el token actual a string_tokens\n",
    "            string_tokens += [token]\n",
    "            # Actualizamos posicionandonos despues del ultimo token actual\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        # Por ultimo con la subcadena restante la damos como input en nuestra funcion recursiva\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        # Cuando ya esta todo tokenizado, la funcion\n",
    "        # devuelve la lista con los tokens\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab('donQuijote.txt')\n",
    "# vocab = get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['\\ufeff', 'e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'm', 'c', 'y', ',', 'r', 'b', 'á', 'ñ', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 61\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 2000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando la tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</w>', 'e', 'a', 'o', 's', 'n', 'r', 'l', 'd', 'u', 'i', 't', 'c', 'm', ',', 'p', 'q', 'y', 'b', 'h', 'v', 'g', 'í', 'j', 'ó', '.', 'f', 'é', 'á', '-', 'z', ';', 'ñ', ':', 'ú', '?', '¿', \"'\", '!', '¡', 'x', '»', '\"', 'ü', '(', ')', '«', '1', '6', 'ï', '0', '4', 'w', 'ù', '\\ufeff', ']', 'à', '7', '5', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "word_given_known = 'seiscientos</w>'\n",
    "word_given_unknown = 'parler</w>'\n",
    "\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra conocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: seiscientos</w>...\n",
      "Tokenization of the known word:\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n",
      "Tokenization treating the known word as unknown:\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n"
     ]
    }
   ],
   "source": [
    "word_given = word_given_known \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra desconocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: parler</w>...\n",
      "Tokenizating of the unknown word:\n",
      "['p', 'a', 'r', 'l', 'e', 'r', '</w>']\n"
     ]
    }
   ],
   "source": [
    "word_given = word_given_unknown \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El costo computacional de la tokenización puede ser bastante alto cuando se trata de textos extensos.\n",
    "La limpieza del conjunto de datos es necesaria si queremos que nuestro tokenizador trabaje de la forma que queremos, de lo contrario aprenderá cosas indeseables.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "42fe2521991ce668210cff6e1c62959905284a5dc1077ee025bbb92f9d2c0e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
