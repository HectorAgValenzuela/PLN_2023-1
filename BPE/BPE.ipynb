{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de BYTE-PAIR ENCODING\n",
    "Integrantes:\n",
    "*   Aguilar Valenzuela Luis Hector\n",
    "*   Camargo Loaiza Julio Andres\n",
    "*   Minjares Neriz Victor Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Y o , </w>': 1,\n",
       "             'J u a n </w>': 1,\n",
       "             'G a l l o </w>': 1,\n",
       "             'd e </w>': 8,\n",
       "             'A n d r a d a , </w>': 1,\n",
       "             'e s c r i b a n o </w>': 1,\n",
       "             'C á m a r a </w>': 1,\n",
       "             'd e l </w>': 4,\n",
       "             'R e y </w>': 1,\n",
       "             'n u e s t r o </w>': 1,\n",
       "             's e ñ o r , </w>': 1,\n",
       "             'l o s </w>': 2,\n",
       "             'q u e </w>': 6,\n",
       "             'r e s i d e n </w>': 1,\n",
       "             'e n </w>': 4,\n",
       "             's u </w>': 1,\n",
       "             'C o n s e j o , </w>': 1,\n",
       "             'c e r t i f i c o </w>': 1,\n",
       "             'y </w>': 10,\n",
       "             'd o y </w>': 1,\n",
       "             'f e </w>': 1,\n",
       "             'q u e , </w>': 1,\n",
       "             'h a b i e n d o </w>': 1,\n",
       "             'v i s t o </w>': 1,\n",
       "             'p o r </w>': 2,\n",
       "             's e ñ o r e s </w>': 1,\n",
       "             'd é l </w>': 1,\n",
       "             'u n </w>': 1,\n",
       "             'l i b r o </w>': 3,\n",
       "             'i n t i t u l a d o </w>': 1,\n",
       "             'E l </w>': 1,\n",
       "             'i n g e n i o s o </w>': 1,\n",
       "             'h i d a l g o </w>': 1,\n",
       "             'l a </w>': 2,\n",
       "             'M a n c h a , </w>': 1,\n",
       "             'c o m p u e s t o </w>': 1,\n",
       "             'M i g u e l </w>': 1,\n",
       "             'C e r v a n t e s </w>': 1,\n",
       "             'S a a v e d r a , </w>': 1,\n",
       "             't a s a r o n </w>': 1,\n",
       "             'c a d a </w>': 1,\n",
       "             'p l i e g o </w>': 1,\n",
       "             'd i c h o </w>': 4,\n",
       "             'a </w>': 3,\n",
       "             't r e s </w>': 2,\n",
       "             'm a r a v e d í s </w>': 2,\n",
       "             'm e d i o ; </w>': 1,\n",
       "             'e l </w>': 2,\n",
       "             'c u a l </w>': 1,\n",
       "             't i e n e </w>': 1,\n",
       "             'o c h e n t a </w>': 1,\n",
       "             'p l i e g o s , </w>': 1,\n",
       "             'a l </w>': 2,\n",
       "             'p r e c i o </w>': 2,\n",
       "             'm o n t a </w>': 1,\n",
       "             'd o c i e n t o s </w>': 1,\n",
       "             'n o v e n t a </w>': 1,\n",
       "             'm e d i o , </w>': 1,\n",
       "             's e </w>': 4,\n",
       "             'h a </w>': 1,\n",
       "             'v e n d e r </w>': 2,\n",
       "             'p a p e l ; </w>': 1,\n",
       "             'd i e r o n </w>': 1,\n",
       "             'l i c e n c i a </w>': 1,\n",
       "             'p a r a </w>': 2,\n",
       "             'e s t e </w>': 1,\n",
       "             'p u e d a </w>': 2,\n",
       "             'v e n d e r , </w>': 1,\n",
       "             'm a n d a r o n </w>': 1,\n",
       "             'e s t a </w>': 1,\n",
       "             't a s a </w>': 1,\n",
       "             'p o n g a </w>': 1,\n",
       "             'p r i n c i p i o </w>': 1,\n",
       "             'l i b r o , </w>': 1,\n",
       "             'n o </w>': 1,\n",
       "             's i n </w>': 1,\n",
       "             'e l l a . </w>': 1,\n",
       "             'Y , </w>': 1,\n",
       "             'd e l l o </w>': 1,\n",
       "             'c o n s t e , </w>': 1,\n",
       "             'd i </w>': 1,\n",
       "             'p r e s e n t e </w>': 1,\n",
       "             'V a l l a d o l i d , </w>': 1,\n",
       "             'v e i n t e </w>': 1,\n",
       "             'd í a s </w>': 1,\n",
       "             'm e s </w>': 1,\n",
       "             'd e c i e m b r e </w>': 1,\n",
       "             'm i l </w>': 1,\n",
       "             's e i s c i e n t o s </w>': 1,\n",
       "             'c u a t r o </w>': 1,\n",
       "             'a ñ o s . </w>': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio. \n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>.\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez.\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "    return vocab\n",
    "\n",
    "get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'o', ',', '</w>']\n",
      "hola\n",
      "['J', 'u', 'a', 'n', '</w>']\n",
      "hola\n",
      "['G', 'a', 'l', 'l', 'o', '</w>']\n",
      "hola\n",
      "['d', 'e', '</w>']\n",
      "hola\n",
      "['A', 'n', 'd', 'r', 'a', 'd', 'a', ',', '</w>']\n",
      "hola\n",
      "['e', 's', 'c', 'r', 'i', 'b', 'a', 'n', 'o', '</w>']\n",
      "hola\n",
      "['C', 'á', 'm', 'a', 'r', 'a', '</w>']\n",
      "hola\n",
      "['d', 'e', 'l', '</w>']\n",
      "hola\n",
      "['R', 'e', 'y', '</w>']\n",
      "hola\n",
      "['n', 'u', 'e', 's', 't', 'r', 'o', '</w>']\n",
      "hola\n",
      "['s', 'e', 'ñ', 'o', 'r', ',', '</w>']\n",
      "hola\n",
      "['l', 'o', 's', '</w>']\n",
      "hola\n",
      "['q', 'u', 'e', '</w>']\n",
      "hola\n",
      "['r', 'e', 's', 'i', 'd', 'e', 'n', '</w>']\n",
      "hola\n",
      "['e', 'n', '</w>']\n",
      "hola\n",
      "['s', 'u', '</w>']\n",
      "hola\n",
      "['C', 'o', 'n', 's', 'e', 'j', 'o', ',', '</w>']\n",
      "hola\n",
      "['c', 'e', 'r', 't', 'i', 'f', 'i', 'c', 'o', '</w>']\n",
      "hola\n",
      "['y', '</w>']\n",
      "hola\n",
      "['d', 'o', 'y', '</w>']\n",
      "hola\n",
      "['f', 'e', '</w>']\n",
      "hola\n",
      "['q', 'u', 'e', ',', '</w>']\n",
      "hola\n",
      "['h', 'a', 'b', 'i', 'e', 'n', 'd', 'o', '</w>']\n",
      "hola\n",
      "['v', 'i', 's', 't', 'o', '</w>']\n",
      "hola\n",
      "['p', 'o', 'r', '</w>']\n",
      "hola\n",
      "['s', 'e', 'ñ', 'o', 'r', 'e', 's', '</w>']\n",
      "hola\n",
      "['d', 'é', 'l', '</w>']\n",
      "hola\n",
      "['u', 'n', '</w>']\n",
      "hola\n",
      "['l', 'i', 'b', 'r', 'o', '</w>']\n",
      "hola\n",
      "['i', 'n', 't', 'i', 't', 'u', 'l', 'a', 'd', 'o', '</w>']\n",
      "hola\n",
      "['E', 'l', '</w>']\n",
      "hola\n",
      "['i', 'n', 'g', 'e', 'n', 'i', 'o', 's', 'o', '</w>']\n",
      "hola\n",
      "['h', 'i', 'd', 'a', 'l', 'g', 'o', '</w>']\n",
      "hola\n",
      "['l', 'a', '</w>']\n",
      "hola\n",
      "['M', 'a', 'n', 'c', 'h', 'a', ',', '</w>']\n",
      "hola\n",
      "['c', 'o', 'm', 'p', 'u', 'e', 's', 't', 'o', '</w>']\n",
      "hola\n",
      "['M', 'i', 'g', 'u', 'e', 'l', '</w>']\n",
      "hola\n",
      "['C', 'e', 'r', 'v', 'a', 'n', 't', 'e', 's', '</w>']\n",
      "hola\n",
      "['S', 'a', 'a', 'v', 'e', 'd', 'r', 'a', ',', '</w>']\n",
      "hola\n",
      "['t', 'a', 's', 'a', 'r', 'o', 'n', '</w>']\n",
      "hola\n",
      "['c', 'a', 'd', 'a', '</w>']\n",
      "hola\n",
      "['p', 'l', 'i', 'e', 'g', 'o', '</w>']\n",
      "hola\n",
      "['d', 'i', 'c', 'h', 'o', '</w>']\n",
      "hola\n",
      "['a', '</w>']\n",
      "hola\n",
      "['t', 'r', 'e', 's', '</w>']\n",
      "hola\n",
      "['m', 'a', 'r', 'a', 'v', 'e', 'd', 'í', 's', '</w>']\n",
      "hola\n",
      "['m', 'e', 'd', 'i', 'o', ';', '</w>']\n",
      "hola\n",
      "['e', 'l', '</w>']\n",
      "hola\n",
      "['c', 'u', 'a', 'l', '</w>']\n",
      "hola\n",
      "['t', 'i', 'e', 'n', 'e', '</w>']\n",
      "hola\n",
      "['o', 'c', 'h', 'e', 'n', 't', 'a', '</w>']\n",
      "hola\n",
      "['p', 'l', 'i', 'e', 'g', 'o', 's', ',', '</w>']\n",
      "hola\n",
      "['a', 'l', '</w>']\n",
      "hola\n",
      "['p', 'r', 'e', 'c', 'i', 'o', '</w>']\n",
      "hola\n",
      "['m', 'o', 'n', 't', 'a', '</w>']\n",
      "hola\n",
      "['d', 'o', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n",
      "hola\n",
      "['n', 'o', 'v', 'e', 'n', 't', 'a', '</w>']\n",
      "hola\n",
      "['m', 'e', 'd', 'i', 'o', ',', '</w>']\n",
      "hola\n",
      "['s', 'e', '</w>']\n",
      "hola\n",
      "['h', 'a', '</w>']\n",
      "hola\n",
      "['v', 'e', 'n', 'd', 'e', 'r', '</w>']\n",
      "hola\n",
      "['p', 'a', 'p', 'e', 'l', ';', '</w>']\n",
      "hola\n",
      "['d', 'i', 'e', 'r', 'o', 'n', '</w>']\n",
      "hola\n",
      "['l', 'i', 'c', 'e', 'n', 'c', 'i', 'a', '</w>']\n",
      "hola\n",
      "['p', 'a', 'r', 'a', '</w>']\n",
      "hola\n",
      "['e', 's', 't', 'e', '</w>']\n",
      "hola\n",
      "['p', 'u', 'e', 'd', 'a', '</w>']\n",
      "hola\n",
      "['v', 'e', 'n', 'd', 'e', 'r', ',', '</w>']\n",
      "hola\n",
      "['m', 'a', 'n', 'd', 'a', 'r', 'o', 'n', '</w>']\n",
      "hola\n",
      "['e', 's', 't', 'a', '</w>']\n",
      "hola\n",
      "['t', 'a', 's', 'a', '</w>']\n",
      "hola\n",
      "['p', 'o', 'n', 'g', 'a', '</w>']\n",
      "hola\n",
      "['p', 'r', 'i', 'n', 'c', 'i', 'p', 'i', 'o', '</w>']\n",
      "hola\n",
      "['l', 'i', 'b', 'r', 'o', ',', '</w>']\n",
      "hola\n",
      "['n', 'o', '</w>']\n",
      "hola\n",
      "['s', 'i', 'n', '</w>']\n",
      "hola\n",
      "['e', 'l', 'l', 'a', '.', '</w>']\n",
      "hola\n",
      "['Y', ',', '</w>']\n",
      "hola\n",
      "['d', 'e', 'l', 'l', 'o', '</w>']\n",
      "hola\n",
      "['c', 'o', 'n', 's', 't', 'e', ',', '</w>']\n",
      "hola\n",
      "['d', 'i', '</w>']\n",
      "hola\n",
      "['p', 'r', 'e', 's', 'e', 'n', 't', 'e', '</w>']\n",
      "hola\n",
      "['V', 'a', 'l', 'l', 'a', 'd', 'o', 'l', 'i', 'd', ',', '</w>']\n",
      "hola\n",
      "['v', 'e', 'i', 'n', 't', 'e', '</w>']\n",
      "hola\n",
      "['d', 'í', 'a', 's', '</w>']\n",
      "hola\n",
      "['m', 'e', 's', '</w>']\n",
      "hola\n",
      "['d', 'e', 'c', 'i', 'e', 'm', 'b', 'r', 'e', '</w>']\n",
      "hola\n",
      "['m', 'i', 'l', '</w>']\n",
      "hola\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n",
      "hola\n",
      "['c', 'u', 'a', 't', 'r', 'o', '</w>']\n",
      "hola\n",
      "['a', 'ñ', 'o', 's', '.', '</w>']\n",
      "hola\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Y': 2,\n",
       "             'o': 53,\n",
       "             ',': 14,\n",
       "             '</w>': 139,\n",
       "             'J': 1,\n",
       "             'u': 18,\n",
       "             'a': 56,\n",
       "             'n': 39,\n",
       "             'G': 1,\n",
       "             'l': 36,\n",
       "             'd': 44,\n",
       "             'e': 90,\n",
       "             'A': 1,\n",
       "             'r': 34,\n",
       "             's': 37,\n",
       "             'c': 22,\n",
       "             'i': 42,\n",
       "             'b': 7,\n",
       "             'C': 3,\n",
       "             'á': 1,\n",
       "             'm': 11,\n",
       "             'R': 1,\n",
       "             'y': 12,\n",
       "             't': 23,\n",
       "             'ñ': 3,\n",
       "             'q': 7,\n",
       "             'j': 1,\n",
       "             'f': 2,\n",
       "             'h': 9,\n",
       "             'v': 10,\n",
       "             'p': 17,\n",
       "             'é': 1,\n",
       "             'E': 1,\n",
       "             'g': 6,\n",
       "             'M': 2,\n",
       "             'S': 1,\n",
       "             'í': 3,\n",
       "             ';': 2,\n",
       "             '.': 2,\n",
       "             'V': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens(vocab):\n",
    "\n",
    "    # Declara un diccionario vacío\n",
    "    tokens = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        word_tokens = word.split()\n",
    "        print(word_tokens)\n",
    "        print(\"hola\")\n",
    "        # Llenamos el diccionario tokens con cada letra y su respectiva frecuencia\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "get_tokens(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'o', ',', '</w>']\n",
      "['J', 'u', 'a', 'n', '</w>']\n",
      "['G', 'a', 'l', 'l', 'o', '</w>']\n",
      "['d', 'e', '</w>']\n",
      "['A', 'n', 'd', 'r', 'a', 'd', 'a', ',', '</w>']\n",
      "['e', 's', 'c', 'r', 'i', 'b', 'a', 'n', 'o', '</w>']\n",
      "['C', 'á', 'm', 'a', 'r', 'a', '</w>']\n",
      "['d', 'e', 'l', '</w>']\n",
      "['R', 'e', 'y', '</w>']\n",
      "['n', 'u', 'e', 's', 't', 'r', 'o', '</w>']\n",
      "['s', 'e', 'ñ', 'o', 'r', ',', '</w>']\n",
      "['l', 'o', 's', '</w>']\n",
      "['q', 'u', 'e', '</w>']\n",
      "['r', 'e', 's', 'i', 'd', 'e', 'n', '</w>']\n",
      "['e', 'n', '</w>']\n",
      "['s', 'u', '</w>']\n",
      "['C', 'o', 'n', 's', 'e', 'j', 'o', ',', '</w>']\n",
      "['c', 'e', 'r', 't', 'i', 'f', 'i', 'c', 'o', '</w>']\n",
      "['y', '</w>']\n",
      "['d', 'o', 'y', '</w>']\n",
      "['f', 'e', '</w>']\n",
      "['q', 'u', 'e', ',', '</w>']\n",
      "['h', 'a', 'b', 'i', 'e', 'n', 'd', 'o', '</w>']\n",
      "['v', 'i', 's', 't', 'o', '</w>']\n",
      "['p', 'o', 'r', '</w>']\n",
      "['s', 'e', 'ñ', 'o', 'r', 'e', 's', '</w>']\n",
      "['d', 'é', 'l', '</w>']\n",
      "['u', 'n', '</w>']\n",
      "['l', 'i', 'b', 'r', 'o', '</w>']\n",
      "['i', 'n', 't', 'i', 't', 'u', 'l', 'a', 'd', 'o', '</w>']\n",
      "['E', 'l', '</w>']\n",
      "['i', 'n', 'g', 'e', 'n', 'i', 'o', 's', 'o', '</w>']\n",
      "['h', 'i', 'd', 'a', 'l', 'g', 'o', '</w>']\n",
      "['l', 'a', '</w>']\n",
      "['M', 'a', 'n', 'c', 'h', 'a', ',', '</w>']\n",
      "['c', 'o', 'm', 'p', 'u', 'e', 's', 't', 'o', '</w>']\n",
      "['M', 'i', 'g', 'u', 'e', 'l', '</w>']\n",
      "['C', 'e', 'r', 'v', 'a', 'n', 't', 'e', 's', '</w>']\n",
      "['S', 'a', 'a', 'v', 'e', 'd', 'r', 'a', ',', '</w>']\n",
      "['t', 'a', 's', 'a', 'r', 'o', 'n', '</w>']\n",
      "['c', 'a', 'd', 'a', '</w>']\n",
      "['p', 'l', 'i', 'e', 'g', 'o', '</w>']\n",
      "['d', 'i', 'c', 'h', 'o', '</w>']\n",
      "['a', '</w>']\n",
      "['t', 'r', 'e', 's', '</w>']\n",
      "['m', 'a', 'r', 'a', 'v', 'e', 'd', 'í', 's', '</w>']\n",
      "['m', 'e', 'd', 'i', 'o', ';', '</w>']\n",
      "['e', 'l', '</w>']\n",
      "['c', 'u', 'a', 'l', '</w>']\n",
      "['t', 'i', 'e', 'n', 'e', '</w>']\n",
      "['o', 'c', 'h', 'e', 'n', 't', 'a', '</w>']\n",
      "['p', 'l', 'i', 'e', 'g', 'o', 's', ',', '</w>']\n",
      "['a', 'l', '</w>']\n",
      "['p', 'r', 'e', 'c', 'i', 'o', '</w>']\n",
      "['m', 'o', 'n', 't', 'a', '</w>']\n",
      "['d', 'o', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n",
      "['n', 'o', 'v', 'e', 'n', 't', 'a', '</w>']\n",
      "['m', 'e', 'd', 'i', 'o', ',', '</w>']\n",
      "['s', 'e', '</w>']\n",
      "['h', 'a', '</w>']\n",
      "['v', 'e', 'n', 'd', 'e', 'r', '</w>']\n",
      "['p', 'a', 'p', 'e', 'l', ';', '</w>']\n",
      "['d', 'i', 'e', 'r', 'o', 'n', '</w>']\n",
      "['l', 'i', 'c', 'e', 'n', 'c', 'i', 'a', '</w>']\n",
      "['p', 'a', 'r', 'a', '</w>']\n",
      "['e', 's', 't', 'e', '</w>']\n",
      "['p', 'u', 'e', 'd', 'a', '</w>']\n",
      "['v', 'e', 'n', 'd', 'e', 'r', ',', '</w>']\n",
      "['m', 'a', 'n', 'd', 'a', 'r', 'o', 'n', '</w>']\n",
      "['e', 's', 't', 'a', '</w>']\n",
      "['t', 'a', 's', 'a', '</w>']\n",
      "['p', 'o', 'n', 'g', 'a', '</w>']\n",
      "['p', 'r', 'i', 'n', 'c', 'i', 'p', 'i', 'o', '</w>']\n",
      "['l', 'i', 'b', 'r', 'o', ',', '</w>']\n",
      "['n', 'o', '</w>']\n",
      "['s', 'i', 'n', '</w>']\n",
      "['e', 'l', 'l', 'a', '.', '</w>']\n",
      "['Y', ',', '</w>']\n",
      "['d', 'e', 'l', 'l', 'o', '</w>']\n",
      "['c', 'o', 'n', 's', 't', 'e', ',', '</w>']\n",
      "['d', 'i', '</w>']\n",
      "['p', 'r', 'e', 's', 'e', 'n', 't', 'e', '</w>']\n",
      "['V', 'a', 'l', 'l', 'a', 'd', 'o', 'l', 'i', 'd', ',', '</w>']\n",
      "['v', 'e', 'i', 'n', 't', 'e', '</w>']\n",
      "['d', 'í', 'a', 's', '</w>']\n",
      "['m', 'e', 's', '</w>']\n",
      "['d', 'e', 'c', 'i', 'e', 'm', 'b', 'r', 'e', '</w>']\n",
      "['m', 'i', 'l', '</w>']\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n",
      "['c', 'u', 'a', 't', 'r', 'o', '</w>']\n",
      "['a', 'ñ', 'o', 's', '.', '</w>']\n",
      "['Y']\n",
      "['o']\n",
      "[',']\n",
      "['</w>']\n",
      "['J']\n",
      "['u']\n",
      "['a']\n",
      "['n']\n",
      "['G']\n",
      "['l']\n",
      "['d']\n",
      "['e']\n",
      "['A']\n",
      "['r']\n",
      "['s']\n",
      "['c']\n",
      "['i']\n",
      "['b']\n",
      "['C']\n",
      "['á']\n",
      "['m']\n",
      "['R']\n",
      "['y']\n",
      "['t']\n",
      "['ñ']\n",
      "['q']\n",
      "['j']\n",
      "['f']\n",
      "['h']\n",
      "['v']\n",
      "['p']\n",
      "['é']\n",
      "['E']\n",
      "['g']\n",
      "['M']\n",
      "['S']\n",
      "['í']\n",
      "[';']\n",
      "['.']\n",
      "['V']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        print(symbols)\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "get_stats(get_tokens(get_vocab('miniCorpus.txt')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair, v_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minicorpus a usar :\n",
    "# https://drive.google.com/file/d/17h_rLrWL2xg3jD0U1CCseeaAd6t17yc0/view?usp=share_linket \n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')\n",
    "\n",
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "758c5f148b5179ab4306bbd5e05355643a81964c7db31e598f4e04b93577a898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
