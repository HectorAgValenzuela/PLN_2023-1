{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de BYTE-PAIR ENCODING\n",
    "Integrantes:\n",
    "*   Aguilar Valenzuela Luis Hector\n",
    "*   Camargo Loaiza Julio Andres\n",
    "*   Minjares Neriz Victor Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Learning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Y o , </w>': 1,\n",
       "             'J u a n </w>': 1,\n",
       "             'G a l l o </w>': 1,\n",
       "             'd e </w>': 8,\n",
       "             'A n d r a d a , </w>': 1,\n",
       "             'e s c r i b a n o </w>': 1,\n",
       "             'C á m a r a </w>': 1,\n",
       "             'd e l </w>': 4,\n",
       "             'R e y </w>': 1,\n",
       "             'n u e s t r o </w>': 1,\n",
       "             's e ñ o r , </w>': 1,\n",
       "             'l o s </w>': 2,\n",
       "             'q u e </w>': 6,\n",
       "             'r e s i d e n </w>': 1,\n",
       "             'e n </w>': 4,\n",
       "             's u </w>': 1,\n",
       "             'C o n s e j o , </w>': 1,\n",
       "             'c e r t i f i c o </w>': 1,\n",
       "             'y </w>': 10,\n",
       "             'd o y </w>': 1,\n",
       "             'f e </w>': 1,\n",
       "             'q u e , </w>': 1,\n",
       "             'h a b i e n d o </w>': 1,\n",
       "             'v i s t o </w>': 1,\n",
       "             'p o r </w>': 2,\n",
       "             's e ñ o r e s </w>': 1,\n",
       "             'd é l </w>': 1,\n",
       "             'u n </w>': 1,\n",
       "             'l i b r o </w>': 3,\n",
       "             'i n t i t u l a d o </w>': 1,\n",
       "             'E l </w>': 1,\n",
       "             'i n g e n i o s o </w>': 1,\n",
       "             'h i d a l g o </w>': 1,\n",
       "             'l a </w>': 2,\n",
       "             'M a n c h a , </w>': 1,\n",
       "             'c o m p u e s t o </w>': 1,\n",
       "             'M i g u e l </w>': 1,\n",
       "             'C e r v a n t e s </w>': 1,\n",
       "             'S a a v e d r a , </w>': 1,\n",
       "             't a s a r o n </w>': 1,\n",
       "             'c a d a </w>': 1,\n",
       "             'p l i e g o </w>': 1,\n",
       "             'd i c h o </w>': 4,\n",
       "             'a </w>': 3,\n",
       "             't r e s </w>': 2,\n",
       "             'm a r a v e d í s </w>': 2,\n",
       "             'm e d i o ; </w>': 1,\n",
       "             'e l </w>': 2,\n",
       "             'c u a l </w>': 1,\n",
       "             't i e n e </w>': 1,\n",
       "             'o c h e n t a </w>': 1,\n",
       "             'p l i e g o s , </w>': 1,\n",
       "             'a l </w>': 2,\n",
       "             'p r e c i o </w>': 2,\n",
       "             'm o n t a </w>': 1,\n",
       "             'd o c i e n t o s </w>': 1,\n",
       "             'n o v e n t a </w>': 1,\n",
       "             'm e d i o , </w>': 1,\n",
       "             's e </w>': 4,\n",
       "             'h a </w>': 1,\n",
       "             'v e n d e r </w>': 2,\n",
       "             'p a p e l ; </w>': 1,\n",
       "             'd i e r o n </w>': 1,\n",
       "             'l i c e n c i a </w>': 1,\n",
       "             'p a r a </w>': 2,\n",
       "             'e s t e </w>': 1,\n",
       "             'p u e d a </w>': 2,\n",
       "             'v e n d e r , </w>': 1,\n",
       "             'm a n d a r o n </w>': 1,\n",
       "             'e s t a </w>': 1,\n",
       "             't a s a </w>': 1,\n",
       "             'p o n g a </w>': 1,\n",
       "             'p r i n c i p i o </w>': 1,\n",
       "             'l i b r o , </w>': 1,\n",
       "             'n o </w>': 1,\n",
       "             's i n </w>': 1,\n",
       "             'e l l a . </w>': 1,\n",
       "             'Y , </w>': 1,\n",
       "             'd e l l o </w>': 1,\n",
       "             'c o n s t e , </w>': 1,\n",
       "             'd i </w>': 1,\n",
       "             'p r e s e n t e </w>': 1,\n",
       "             'V a l l a d o l i d , </w>': 1,\n",
       "             'v e i n t e </w>': 1,\n",
       "             'd í a s </w>': 1,\n",
       "             'm e s </w>': 1,\n",
       "             'd e c i e m b r e </w>': 1,\n",
       "             'm i l </w>': 1,\n",
       "             's e i s c i e n t o s </w>': 1,\n",
       "             'c u a t r o </w>': 1,\n",
       "             'a ñ o s . </w>': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio. \n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>.\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez.\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "    return vocab\n",
    "\n",
    "get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens(vocab)\n",
    "Funcion que recibe un diccionario de palabras, despues transforma el diccionario de palabras a uno de letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(vocab):\n",
    "\n",
    "    # Declara un diccionario vacío\n",
    "    tokens = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario tokens con cada letra y su respectiva frecuencia\n",
    "        for token in word_tokens:\n",
    "            # Guarda la palabra en el diccionario y le suma la frecuencia de la palabra.\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "# get_tokens(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "# get_stats(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair, v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas par ejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minicorpus a usar :\n",
    "# https://drive.google.com/file/d/17h_rLrWL2xg3jD0U1CCseeaAd6t17yc0/view?usp=share_linket \n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 22, ',</w>': 14, 'J': 1, 'u': 11, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 18, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 52\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# En este bloque de código se muestran un diccionario de todas las palabras en el vocabulario.\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('o', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 115, 'J': 1, 'u': 18, 'a': 56, 'n': 39, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e': 90, 'A': 1, 'r': 34, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 41\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('e', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 91, 'J': 1, 'u': 18, 'a': 56, 'n': 39, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 66, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 42\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('a', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 72, 'J': 1, 'u': 18, 'a': 37, 'n': 39, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 66, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 43\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: ('e', 'n')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 72, 'J': 1, 'u': 18, 'a': 37, 'n': 22, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 49, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 44\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: (',', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 58, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 49, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 44\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('l', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 45, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 49, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'l</w>': 13, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 45\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('e', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 45, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'e': 37, 'l</w>': 13, 'R': 1, 'y': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 46\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('y', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'e': 37, 'l</w>': 13, 'R': 1, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 46\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('d', 'e')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 35, 'e</w>': 24, 'A': 1, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 47\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('d', 'e</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 23, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 27, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 48\n",
      "==========\n",
      "Iter: 10\n",
      "Best pair: ('l', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 27, 'r': 34, 'es': 12, 'c': 22, 'i': 34, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 49\n",
      "==========\n",
      "Iter: 11\n",
      "Best pair: ('d', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 34, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 50\n",
      "==========\n",
      "Iter: 12\n",
      "Best pair: ('a', 'r')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 51\n",
      "==========\n",
      "Iter: 13\n",
      "Best pair: ('o', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 22, ',</w>': 14, 'J': 1, 'u': 18, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 18, 'ñ': 3, 'os': 7, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 52\n",
      "==========\n",
      "Iter: 14\n",
      "Best pair: ('q', 'u')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 22, ',</w>': 14, 'J': 1, 'u': 11, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 18, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 52\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Decoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio. \n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>.\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez.\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair,v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas par ejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens_from_vocab(vocab)\n",
    "Funcion que toma como entrada un diccionario con las frecuencias(vocab) y te regresa dos diccionarios. tokens_frequencies es un diccionario de letras/simbolos con sus frecuencias de aparicion en el vocab. \n",
    "vocab_tokenization es un diccionario con las palabras del vocab con su respectiva tokenizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    # Creamos diccionarios vacios\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separamos la palabra en sus letras\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario con las letras/simbolos y sus frecuencias\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        # En este otro lo llenamos con las palabras y su tokenizacion\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  measure_token_length(token)\n",
    "Función que mide la longitud de un token de texto, incluyendo o no un espacio en blanco dependiendo de si el token termina con \"\\</w>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token):\n",
    "    # Verificamos si el token es el final de una palabra, esto es\n",
    "    # si los ultimos 4 caracteres son </w>\n",
    "    if token[-4:] == '</w>':\n",
    "        # Si lo es, entonces devolvemos la longitud sin esos 4 \n",
    "        # caracteres y le agregamos un espacio en blanco\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        # De lo contrario, devolvemos la longitud tal cual\n",
    "        return len(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_word(string, sorted_tokens, unknown_token='\\</u>')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que tokeniza una cadena de texto dada, string, en base a una lista de tokens ordenados, sorted_tokens. \\</u> es la expresion regular para token desconocido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    #  Si la cadena a tokenizar es un espacio en blanco o una palabra \n",
    "    # fuera del vocabulario regresamos una lista vacia o con la palabra\n",
    "    # desconocida\n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    # Arreglo donde guardaremos los tokens\n",
    "    string_tokens = []\n",
    "    # Iteramos por cada letra/simbolo del vocabulario\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        # Agarramos un elemento del vocabulario\n",
    "        token = sorted_tokens[i]\n",
    "        # Evitamos que se tomen caracteres del token como\n",
    "        # caracteres especiales de expresiones regulares\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        # Se busca todas las coincidencias que tiene el token, token_reg, en la cadena de \n",
    "        # texto, string, y se almacenan sus posiciones inicial y final\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        # Si no hay coincidencias pasamos al siguiente token    \n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "\n",
    "        # Agarramos las posiciones finales de todas las coincidencias e inicializamos\n",
    "        # el comienzo en 0, esto es el principio del string a tokenizar\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "        substring_start_position = 0\n",
    "        # Iteramos sobre todas las posiciones finales de las coincidencias\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # Dividiremos la cadena en una subcadena\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            # Llamamos recursivamente para dividir la subcadena en tokens con una \n",
    "            # nueva lista de tokens que contiene todos los tokens en sorted_tokens excepto los \n",
    "            # que ya han sido utilizados\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            # Agregamos el token actual a string_tokens\n",
    "            string_tokens += [token]\n",
    "            # Actualizamos posicionandonos despues del ultimo token actual\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        # Por ultimo con la subcadena restante la damos como input en nuestra funcion recursiva\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        # Cuando ya esta todo tokenizado, la funcion\n",
    "        # devuelve la lista con los tokens\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 10000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando la tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_given_known = 'mountains</w>'\n",
    "word_given_unknown = 'Ilikeeatingapples!</w>'\n",
    "\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra conocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_given = word_given_known \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra desconocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_given = word_given_unknown \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "758c5f148b5179ab4306bbd5e05355643a81964c7db31e598f4e04b93577a898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
