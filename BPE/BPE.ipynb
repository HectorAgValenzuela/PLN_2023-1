{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de BYTE-PAIR ENCODING\n",
    "Integrantes:\n",
    "*   Aguilar Valenzuela Luis Hector\n",
    "*   Camargo Loaiza Julio Andres\n",
    "*   Minjares Neriz Victor Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Learning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens(vocab)\n",
    "Funcion que recibe un diccionario de palabras, despues transforma el diccionario de palabras a uno de letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(vocab):\n",
    "\n",
    "    # Declara un diccionario vacío\n",
    "    tokens = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario tokens con cada letra y su respectiva frecuencia\n",
    "        for token in word_tokens:\n",
    "            # Guarda la palabra en el diccionario y le suma la frecuencia de la palabra.\n",
    "            tokens[token] += freq\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair, v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas parejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minicorpus a usar :\n",
    "# https://drive.google.com/file/d/17h_rLrWL2xg3jD0U1CCseeaAd6t17yc0/view?usp=share_linket \n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 53, ',': 14, '</w>': 139, 'j': 2, 'u': 18, 'a': 57, 'n': 39, 'g': 7, 'l': 36, 'd': 44, 'e': 91, 'r': 35, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 't': 23, 'ñ': 3, 'q': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 30\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# En este bloque de código se muestran un diccionario de todas las palabras en el vocabulario.\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('e', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',': 14, '</w>': 91, 'j': 2, 'u': 18, 'a': 57, 'n': 39, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 67, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 't': 23, 'ñ': 3, 'q': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 32\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('a', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',': 14, '</w>': 72, 'j': 2, 'u': 18, 'a': 38, 'n': 39, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 67, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 't': 23, 'ñ': 3, 'q': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 33\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('e', 'n')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',': 14, '</w>': 72, 'j': 2, 'u': 18, 'a': 38, 'n': 22, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 50, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 34\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: (',', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 58, 'g': 7, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 50, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 34\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: ('l', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 45, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'e': 50, 's': 38, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'l</w>': 13, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 35\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('e', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 14, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 45, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'e': 38, 'l</w>': 13, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 36\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('y', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'e': 38, 'l</w>': 13, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 37\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('d', 'e')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 23, 'o</w>': 24, 'd': 35, 'e</w>': 24, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 38\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('d', 'e</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 23, 'o</w>': 24, 'de</w>': 8, 'd': 27, 'r': 35, 'es': 12, 'c': 25, 'i': 42, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 39\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('l', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 27, 'r': 35, 'es': 12, 'c': 25, 'i': 34, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 40\n",
      "==========\n",
      "Iter: 10\n",
      "Best pair: ('d', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 38, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 35, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 41\n",
      "==========\n",
      "Iter: 11\n",
      "Best pair: ('a', 'r')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 29, ',</w>': 14, 'j': 2, 'u': 18, 'a': 31, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 26, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 42\n",
      "==========\n",
      "Iter: 12\n",
      "Best pair: ('o', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 22, ',</w>': 14, 'j': 2, 'u': 18, 'a': 31, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 19, 'ñ': 3, 'os': 7, 'q': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 43\n",
      "==========\n",
      "Iter: 13\n",
      "Best pair: ('q', 'u')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 22, ',</w>': 14, 'j': 2, 'u': 11, 'a': 31, 'n': 22, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 19, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 43\n",
      "==========\n",
      "Iter: 14\n",
      "Best pair: ('o', 'n')\n",
      "Tokens: defaultdict(<class 'int'>, {'y': 2, 'o': 15, ',</w>': 14, 'j': 2, 'u': 11, 'a': 31, 'n': 15, '</w>': 33, 'g': 7, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'd': 19, 'r': 28, 'es': 12, 'c': 25, 'i': 26, 'b': 7, 'á': 1, 'm': 13, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'e': 29, 'y</w>': 12, 't': 23, 's': 19, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'on': 7, 'f': 2, 'h': 9, 'v': 11, 'p': 17, 'é': 1, 'li': 8, 'di': 8, 'í': 3, ';': 2, '.': 2})\n",
      "Number of tokens: 44\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>\n",
    "                # NOTA : Hacemos todo minusculas para reducir el tiempo de ejecucion\n",
    "                # y el numero de tokens\n",
    "                # NOTA2 : list() aqui puede ser opcional, tal vez\n",
    "                vocab[' '.join(list(word.lower())) + ' </w>'] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair,v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas parejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens_from_vocab(vocab)\n",
    "Funcion que toma como entrada un diccionario con las frecuencias(vocab) y te regresa dos diccionarios. tokens_frequencies es un diccionario de letras/simbolos con sus frecuencias de aparicion en el vocab. \n",
    "vocab_tokenization es un diccionario con las palabras del vocab con su respectiva tokenizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    # Creamos diccionarios vacios\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separamos la palabra en sus letras\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario con las letras/simbolos y sus frecuencias\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        # En este otro lo llenamos con las palabras y su tokenizacion\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  measure_token_length(token)\n",
    "Función que mide la longitud de un token de texto, incluyendo o no un espacio en blanco dependiendo de si el token termina con \"\\</w>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token):\n",
    "    # Verificamos si el token es el final de una palabra, esto es\n",
    "    # si los ultimos 4 caracteres son </w>\n",
    "    if token[-4:] == '</w>':\n",
    "        # Si lo es, entonces devolvemos la longitud sin esos 4 \n",
    "        # caracteres y le agregamos un espacio en blanco\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        # De lo contrario, devolvemos la longitud tal cual\n",
    "        return len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_word(string, sorted_tokens, unknown_token='\\</u>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que tokeniza una cadena de texto dada, string, en base a una lista de tokens ordenados, sorted_tokens. \\</u> es la expresion regular para token desconocido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    #  Si la cadena a tokenizar es un espacio en blanco o una palabra \n",
    "    # fuera del vocabulario regresamos una lista vacia o con la palabra\n",
    "    # desconocida\n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    # Arreglo donde guardaremos los tokens\n",
    "    string_tokens = []\n",
    "    # Iteramos por cada letra/simbolo del vocabulario\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        # Agarramos un elemento del vocabulario\n",
    "        token = sorted_tokens[i]\n",
    "        # Evitamos que se tomen caracteres del token como\n",
    "        # caracteres especiales de expresiones regulares\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        # Se busca todas las coincidencias que tiene el token, token_reg, en la cadena de \n",
    "        # texto, string, y se almacenan sus posiciones inicial y final\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        # Si no hay coincidencias pasamos al siguiente token    \n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "\n",
    "        # Agarramos las posiciones finales de todas las coincidencias e inicializamos\n",
    "        # el comienzo en 0, esto es el principio del string a tokenizar\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "        substring_start_position = 0\n",
    "        # Iteramos sobre todas las posiciones finales de las coincidencias\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # Dividiremos la cadena en una subcadena\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            # Llamamos recursivamente para dividir la subcadena en tokens con una \n",
    "            # nueva lista de tokens que contiene todos los tokens en sorted_tokens excepto los \n",
    "            # que ya han sido utilizados\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            # Agregamos el token actual a string_tokens\n",
    "            string_tokens += [token]\n",
    "            # Actualizamos posicionandonos despues del ultimo token actual\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        # Por ultimo con la subcadena restante la damos como input en nuestra funcion recursiva\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        # Cuando ya esta todo tokenizado, la funcion\n",
    "        # devuelve la lista con los tokens\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab('donQuijote.txt')\n",
    "# vocab = get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'm', 'c', 'y', ',', 'r', 'b', 'á', 'ñ', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 60\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('e', '</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'e</w>', 'm', 'c', 'y', ',', 'r', 'b', 'á', 'ñ', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 61\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('a', '</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',', 'r', 'b', 'á', 'ñ', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 62\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('o', '</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'o</w>', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',', 'r', 'b', 'á', 'ñ', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 63\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: ('s', '</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'o</w>', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',', 'r', 'b', 'á', 'ñ', 's</w>', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 64\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: (',', '</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'o</w>', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',</w>', 'r', 'b', 'á', 'ñ', 's</w>', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 65\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('e', 'n')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',</w>', 'r', 'b', 'á', 'ñ', 's</w>', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 66\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('q', 'u')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'qu', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',</w>', 'u', 'r', 'b', 'á', 'ñ', 's</w>', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 66\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('e', 'r')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'qu', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',</w>', 'u', 'r', 'b', 'á', 'ñ', 's</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 67\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('e', 's')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'qu', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ñ', 's</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 68\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('qu', 'e</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'qu', 'j', 't', 'e</w>', 'a</w>', 'm', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ñ', 's</w>', 'que</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 69\n",
      "==========\n",
      "Iter: 10\n",
      "Best pair: ('a', 'n')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'qu', 'j', 't', 'e</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ñ', 's</w>', 'que</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 70\n",
      "==========\n",
      "Iter: 11\n",
      "Best pair: ('d', 'e</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'qu', 'j', 't', 'e</w>', 'de</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ñ', 's</w>', 'que</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 71\n",
      "==========\n",
      "Iter: 12\n",
      "Best pair: ('y', '</w>')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'qu', 'j', 't', 'e</w>', 'de</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'y</w>', 'ñ', 's</w>', 'que</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 72\n",
      "==========\n",
      "Iter: 13\n",
      "Best pair: ('o', 'n')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'y</w>', 'ñ', 's</w>', 'que</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 73\n",
      "==========\n",
      "Iter: 14\n",
      "Best pair: ('a', 'r')\n",
      "All tokens: dict_keys(['e', 'l', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ar', 'y</w>', 'ñ', 's</w>', 'que</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 74\n",
      "==========\n",
      "Iter: 15\n",
      "Best pair: ('e', 'l')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'l', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 's</w>', 'que</w>', 'er', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 75\n",
      "==========\n",
      "Iter: 16\n",
      "Best pair: ('o', 's</w>')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'l', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'os</w>', 'que</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 76\n",
      "==========\n",
      "Iter: 17\n",
      "Best pair: ('o', 'r')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'a', 'l', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'a</w>', 'm', 'an', 'c', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 77\n",
      "==========\n",
      "Iter: 18\n",
      "Best pair: ('a', 'l')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'l', 'a</w>', 'm', 'an', 'c', 'a', 'y', ',</w>', 'u', 'r', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 78\n",
      "==========\n",
      "Iter: 19\n",
      "Best pair: ('a', 'd')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'l', 'a</w>', 'm', 'an', 'c', 'a', 'y', ',</w>', 'u', 'r', 'ad', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 79\n",
      "==========\n",
      "Iter: 20\n",
      "Best pair: ('l', 'a</w>')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', ',</w>', 'u', 'l', 'r', 'ad', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 80\n",
      "==========\n",
      "Iter: 21\n",
      "Best pair: ('en', '</w>')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', ',</w>', 'u', 'l', 'r', 'ad', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 81\n",
      "==========\n",
      "Iter: 22\n",
      "Best pair: ('a', 's</w>')\n",
      "All tokens: dict_keys(['el', '</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', ',</w>', 'u', 'l', 'r', 'ad', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 82\n",
      "==========\n",
      "Iter: 23\n",
      "Best pair: ('el', '</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', ',</w>', 'u', 'l', 'r', 'ad', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'í', ';', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 83\n",
      "==========\n",
      "Iter: 24\n",
      "Best pair: ('d', 'i')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', ',</w>', 'u', 'l', 'r', 'ad', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 84\n",
      "==========\n",
      "Iter: 25\n",
      "Best pair: ('o', ',</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', 'o,</w>', 'u', 'l', 'r', 'ad', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 85\n",
      "==========\n",
      "Iter: 26\n",
      "Best pair: ('a', 'b')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', 'o,</w>', 'u', 'l', 'r', 'ad', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'ab', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 86\n",
      "==========\n",
      "Iter: 27\n",
      "Best pair: ('es', 't')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'o', 's', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 'y', 'o,</w>', 'u', 'l', 'r', 'ad', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'ab', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 87\n",
      "==========\n",
      "Iter: 28\n",
      "Best pair: ('o', 's')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 's', 'y', 'o,</w>', 'u', 'l', 'r', 'ad', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'ab', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 88\n",
      "==========\n",
      "Iter: 29\n",
      "Best pair: ('c', 'i')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'a', 's', 'y', 'o,</w>', 'u', 'l', 'r', 'ad', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'ab', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'ci', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 89\n",
      "==========\n",
      "Iter: 30\n",
      "Best pair: ('a', 's')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on', '</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', 'l', 'r', 'ad', 'a', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'er', 'f', 'ab', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'ci', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 90\n",
      "==========\n",
      "Iter: 31\n",
      "Best pair: ('on', '</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 's</w>', 'é', 'di', 'í', ';', 'ci', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 91\n",
      "==========\n",
      "Iter: 32\n",
      "Best pair: ('e', 's</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'di', 'í', 's</w>', ';', 'ci', 'el', '.', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 92\n",
      "==========\n",
      "Iter: 33\n",
      "Best pair: ('.', '</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'di', 'í', 's</w>', ';', 'ci', 'el', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 93\n",
      "==========\n",
      "Iter: 34\n",
      "Best pair: ('u', 'n')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a', ',</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'un', 'di', 'í', 's</w>', ';', 'ci', 'el', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 94\n",
      "==========\n",
      "Iter: 35\n",
      "Best pair: ('a', ',</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a,</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', ',</w>', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'un', 'a', 'di', 'í', 's</w>', ';', 'ci', 'el', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 95\n",
      "==========\n",
      "Iter: 36\n",
      "Best pair: ('en', 't')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a,</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', ',</w>', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'un', 'a', 'di', 'í', 's</w>', ';', 'ent', 'ci', 'el', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 96\n",
      "==========\n",
      "Iter: 37\n",
      "Best pair: ('n', 'o</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a,</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', ',</w>', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'un', 'a', 'di', 'í', 's</w>', ';', 'ent', 'ci', 'el', 'no</w>', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 97\n",
      "==========\n",
      "Iter: 38\n",
      "Best pair: ('m', 'i')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a,</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', ',</w>', 'os</w>', 'que</w>', 'en</w>', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'un', 'mi', 'a', 'di', 'í', 's</w>', ';', 'ent', 'ci', 'el', 'no</w>', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 98\n",
      "==========\n",
      "Iter: 39\n",
      "Best pair: ('s', 'u')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a,</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', ',</w>', 'os</w>', 'que</w>', 'en</w>', 'su', 'on', 'er', 'f', 'ab', 'v', 'p', 'es</w>', 'é', 'un', 'mi', 'a', 'di', 'í', 's</w>', ';', 'ent', 'ci', 'el', 'no</w>', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 99\n",
      "==========\n",
      "Iter: 40\n",
      "Best pair: ('d', 'o</w>')\n",
      "All tokens: dict_keys(['el</w>', 'i', 'n', 'g', 'en', 'os', 'o</w>', 'h', 'd', 'al', 'on</w>', 'qu', 'j', 'o', 't', 'e</w>', 'de</w>', 'la</w>', 'm', 'an', 'c', 'a</w>', 'as', 'y', 'o,</w>', 'u', '</w>', 'l', 'r', 'ad', 'a,</w>', 'es', 'b', 'á', 'ar', 'e', 'y</w>', 'est', 's', 'ñ', 'or', ',</w>', 'os</w>', 'que</w>', 'en</w>', 'su', 'on', 'er', 'f', 'ab', 'do</w>', 'v', 'p', 'es</w>', 'é', 'un', 'mi', 'a', 'di', 'í', 's</w>', ';', 'ent', 'ci', 'el', 'no</w>', '.</w>', 'as</w>', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', '.', 'ü', '»', \"'\", '«', '(', ')', '\"', ',', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 100\n",
      "==========\n",
      "Iter: 41\n",
      "Best pair: ('s', 'i')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mIter: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i))\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest pair: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(best))\n\u001b[1;32m---> 10\u001b[0m tokens_frequencies, vocab_tokenization \u001b[39m=\u001b[39m get_tokens_from_vocab(vocab)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAll tokens: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(tokens_frequencies\u001b[39m.\u001b[39mkeys()))\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of tokens: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(tokens_frequencies\u001b[39m.\u001b[39mkeys())))\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mget_tokens_from_vocab\u001b[1;34m(vocab)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Llenamos el diccionario con las letras/simbolos y sus frecuencias\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m word_tokens:\n\u001b[1;32m---> 11\u001b[0m     tokens_frequencies[token] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m freq\n\u001b[0;32m     12\u001b[0m \u001b[39m# En este otro lo llenamos con las palabras y su tokenizacion\u001b[39;00m\n\u001b[0;32m     13\u001b[0m vocab_tokenization[\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(word_tokens)] \u001b[39m=\u001b[39m word_tokens\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_merges = 2000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando la tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</w>', 'e', 'a', 'o', 's', 'n', 'r', 'l', 'd', 'u', 'i', 't', 'c', 'm', ',', 'p', 'q', 'y', 'b', 'h', 'v', 'g', 'í', 'j', 'ó', '.', 'f', 'é', 'á', '-', 'z', ';', 'ñ', ':', 'ú', '?', '¿', \"'\", '!', '¡', 'x', '»', '\"', 'ü', '(', ')', '«', '1', '6', 'ï', '0', '4', 'w', 'ù', '\\ufeff', ']', 'à', '7', '5', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "word_given_known = 'seiscientos</w>'\n",
    "word_given_unknown = 'parler</w>'\n",
    "\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra conocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: seiscientos</w>...\n",
      "Tokenization of the known word:\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n",
      "Tokenization treating the known word as unknown:\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n"
     ]
    }
   ],
   "source": [
    "word_given = word_given_known \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra desconocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: parler</w>...\n",
      "Tokenizating of the unknown word:\n",
      "['p', 'a', 'r', 'l', 'e', 'r', '</w>']\n"
     ]
    }
   ],
   "source": [
    "word_given = word_given_unknown \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El costo computacional de la tokenización puede ser bastante alto cuando se trata de textos extensos.\n",
    "* La limpieza del conjunto de datos es necesaria si queremos que nuestro tokenizador trabaje de la forma que queremos, de lo contrario aprenderá cosas indeseables.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "42fe2521991ce668210cff6e1c62959905284a5dc1077ee025bbb92f9d2c0e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
