{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de BYTE-PAIR ENCODING\n",
    "Integrantes:\n",
    "*   Aguilar Valenzuela Luis Hector\n",
    "*   Camargo Loaiza Julio Andres\n",
    "*   Minjares Neriz Victor Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Learning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio. \n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>.\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez.\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens(vocab)\n",
    "Funcion que recibe un diccionario de palabras, despues transforma el diccionario de palabras a uno de letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(vocab):\n",
    "\n",
    "    # Declara un diccionario vacío\n",
    "    tokens = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario tokens con cada letra y su respectiva frecuencia\n",
    "        for token in word_tokens:\n",
    "            # Guarda la palabra en el diccionario y le suma la frecuencia de la palabra.\n",
    "            tokens[token] += freq\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair, v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas par ejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minicorpus a usar :\n",
    "# https://drive.google.com/file/d/17h_rLrWL2xg3jD0U1CCseeaAd6t17yc0/view?usp=share_linket \n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "Tokens: defaultdict(<class 'int'>, {'Yo,</w>': 1, 'Juan</w>': 1, 'Gallo</w>': 1, 'de</w>': 8, 'Andrada,</w>': 1, 'escribano</w>': 1, 'Cámara</w>': 1, 'del</w>': 4, 'Rey</w>': 1, 'nuestro</w>': 1, 'señor,</w>': 1, 'los</w>': 2, 'que</w>': 6, 'residen</w>': 1, 'en</w>': 4, 'su</w>': 1, 'Consejo,</w>': 1, 'certifico</w>': 1, 'y</w>': 10, 'doy</w>': 1, 'fe</w>': 1, 'que,</w>': 1, 'habiendo</w>': 1, 'visto</w>': 1, 'por</w>': 2, 'señores</w>': 1, 'dél</w>': 1, 'un</w>': 1, 'libro</w>': 3, 'intitulado</w>': 1, 'El</w>': 1, 'ingenioso</w>': 1, 'hidalgo</w>': 1, 'la</w>': 2, 'Mancha,</w>': 1, 'compuesto</w>': 1, 'Miguel</w>': 1, 'Cervantes</w>': 1, 'Saavedra,</w>': 1, 'tasaron</w>': 1, 'cada</w>': 1, 'pliego</w>': 1, 'dicho</w>': 4, 'a</w>': 3, 'tres</w>': 2, 'maravedís</w>': 2, 'medio;</w>': 1, 'el</w>': 2, 'cual</w>': 1, 'tiene</w>': 1, 'ochenta</w>': 1, 'pliegos,</w>': 1, 'al</w>': 2, 'precio</w>': 2, 'monta</w>': 1, 'docientos</w>': 1, 'noventa</w>': 1, 'medio,</w>': 1, 'se</w>': 4, 'ha</w>': 1, 'vender</w>': 2, 'papel;</w>': 1, 'dieron</w>': 1, 'licencia</w>': 1, 'para</w>': 2, 'este</w>': 1, 'pueda</w>': 2, 'vender,</w>': 1, 'mandaron</w>': 1, 'esta</w>': 1, 'tasa</w>': 1, 'ponga</w>': 1, 'principio</w>': 1, 'libro,</w>': 1, 'no</w>': 1, 'sin</w>': 1, 'ella.</w>': 1, 'Y,</w>': 1, 'dello</w>': 1, 'conste,</w>': 1, 'di</w>': 1, 'presente</w>': 1, 'Valladolid,</w>': 1, 'veinte</w>': 1, 'días</w>': 1, 'mes</w>': 1, 'deciembre</w>': 1, 'mil</w>': 1, 'seiscientos</w>': 1, 'cuatro</w>': 1, 'años.</w>': 1})\n",
      "Number of tokens: 91\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# En este bloque de código se muestran un diccionario de todas las palabras en el vocabulario.\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio. \n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>.\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez.\n",
    "                vocab[' '.join(list(word.lower())) + ' </w>'] += 1 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair,v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas par ejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        # Se remplaza una palabra word por el bigrama \n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens_from_vocab(vocab)\n",
    "Funcion que toma como entrada un diccionario con las frecuencias(vocab) y te regresa dos diccionarios. tokens_frequencies es un diccionario de letras/simbolos con sus frecuencias de aparicion en el vocab. \n",
    "vocab_tokenization es un diccionario con las palabras del vocab con su respectiva tokenizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    # Creamos diccionarios vacios\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separamos la palabra en sus letras\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario con las letras/simbolos y sus frecuencias\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        # En este otro lo llenamos con las palabras y su tokenizacion\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  measure_token_length(token)\n",
    "Función que mide la longitud de un token de texto, incluyendo o no un espacio en blanco dependiendo de si el token termina con \"\\</w>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token):\n",
    "    # Verificamos si el token es el final de una palabra, esto es\n",
    "    # si los ultimos 4 caracteres son </w>\n",
    "    if token[-4:] == '</w>':\n",
    "        # Si lo es, entonces devolvemos la longitud sin esos 4 \n",
    "        # caracteres y le agregamos un espacio en blanco\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        # De lo contrario, devolvemos la longitud tal cual\n",
    "        return len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_word(string, sorted_tokens, unknown_token='\\</u>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que tokeniza una cadena de texto dada, string, en base a una lista de tokens ordenados, sorted_tokens. \\</u> es la expresion regular para token desconocido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    #  Si la cadena a tokenizar es un espacio en blanco o una palabra \n",
    "    # fuera del vocabulario regresamos una lista vacia o con la palabra\n",
    "    # desconocida\n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    # Arreglo donde guardaremos los tokens\n",
    "    string_tokens = []\n",
    "    # Iteramos por cada letra/simbolo del vocabulario\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        # Agarramos un elemento del vocabulario\n",
    "        token = sorted_tokens[i]\n",
    "        # Evitamos que se tomen caracteres del token como\n",
    "        # caracteres especiales de expresiones regulares\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        # Se busca todas las coincidencias que tiene el token, token_reg, en la cadena de \n",
    "        # texto, string, y se almacenan sus posiciones inicial y final\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        # Si no hay coincidencias pasamos al siguiente token    \n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "\n",
    "        # Agarramos las posiciones finales de todas las coincidencias e inicializamos\n",
    "        # el comienzo en 0, esto es el principio del string a tokenizar\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "        substring_start_position = 0\n",
    "        # Iteramos sobre todas las posiciones finales de las coincidencias\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # Dividiremos la cadena en una subcadena\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            # Llamamos recursivamente para dividir la subcadena en tokens con una \n",
    "            # nueva lista de tokens que contiene todos los tokens en sorted_tokens excepto los \n",
    "            # que ya han sido utilizados\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            # Agregamos el token actual a string_tokens\n",
    "            string_tokens += [token]\n",
    "            # Actualizamos posicionandonos despues del ultimo token actual\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        # Por ultimo con la subcadena restante la damos como input en nuestra funcion recursiva\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        # Cuando ya esta todo tokenizado, la funcion\n",
    "        # devuelve la lista con los tokens\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "donQuijote = get_vocab('donQuijote.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['\\ufeff', 'e', 'l', '</w>', 'i', 'n', 'g', 'o', 's', 'h', 'd', 'a', 'q', 'u', 'j', 't', 'm', 'c', 'y', ',', 'r', 'b', 'á', 'ñ', 'f', 'v', 'p', 'é', 'í', ';', '.', 'ó', '1', '6', '0', '4', 'ú', 'z', ':', 'x', '¿', '?', '-', '¡', '!', 'ü', '»', \"'\", '«', '(', ')', '\"', 'ï', 'w', ']', 'à', '7', '5', '2', '3', 'ù'])\n",
      "Number of tokens: 61\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(donQuijote)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 3000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(donQuijote)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, donQuijote)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(donQuijote)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando la tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</w>', 'e', 'a', 'o', 'd', 'i', 'n', 's', 'l', 'r', 't', 'c', 'u', 'p', ',', 'y', 'm', 'v', 'h', 'b', 'q', 'g', 'C', 'ñ', 'í', 'Y', 'f', 'M', ';', '.', 'J', 'G', 'A', 'á', 'R', 'j', 'é', 'E', 'S', 'V']\n"
     ]
    }
   ],
   "source": [
    "word_given_known = 'seiscientos</w>'\n",
    "word_given_unknown = 'parler</w>'\n",
    "\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra conocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: seiscientos</w>...\n",
      "Tokenization of the known word:\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n",
      "Tokenization treating the known word as unknown:\n",
      "['s', 'e', 'i', 's', 'c', 'i', 'e', 'n', 't', 'o', 's', '</w>']\n"
     ]
    }
   ],
   "source": [
    "word_given = word_given_known \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra desconocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: parler</w>...\n",
      "Tokenizating of the unknown word:\n",
      "['p', 'a', 'r', 'l', 'e', 'r', '</w>']\n"
     ]
    }
   ],
   "source": [
    "word_given = word_given_unknown \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "758c5f148b5179ab4306bbd5e05355643a81964c7db31e598f4e04b93577a898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
