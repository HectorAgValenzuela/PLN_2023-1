{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de BYTE-PAIR ENCODING\n",
    "Integrantes:\n",
    "*   Aguilar Valenzuela Luis Hector\n",
    "*   Camargo Loaiza Julio Andres\n",
    "*   Minjares Neriz Victor Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Learning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab(filename)\n",
    "Función que recibe el nombre de el archivo de texto y devuelve un vocabulario de palabras con la frecuencia de cada palabra y un separador en cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    # La funcion defaultdict crea un dictionario vacio. \n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            # La función strip quita los espacios al principio y al final de un string\n",
    "            # La función split separa las palabras y las devuelve en un array\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            # Recorre cada palabra del arreglo de palabras\n",
    "            for word in words:\n",
    "                # Aqui se llena el diccionario. Agregara al diccionario el elemnto : un espacio +\n",
    "                # la palabra + el simbolo de fin de palabra </w>.\n",
    "                # NOTA : list() aqui puede ser opcional, tal vez.\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "    return vocab\n",
    "\n",
    "# get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que recibe un diccionario de palabras, despues transforma el diccionario de palabras a uno de letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(vocab):\n",
    "\n",
    "    # Declara un diccionario vacío\n",
    "    tokens = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        word_tokens = word.split()\n",
    "        # Llenamos el diccionario tokens con cada letra y su respectiva frecuencia\n",
    "        for token in word_tokens:\n",
    "            # Guarda la palabra en el diccionario y le suma la frecuencia de la palabra.\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "# get_tokens(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats(vocab)\n",
    "Función que recibe un vocabulario (un diccionario con la frecuencia de cada palabra) y devuelve un diccionario con la frecuencia de los bigramas (pares de palabras consecutivos) en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    # Declara un diccionario vacío\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    # Iteramos por cada palabra y tomando su respectiva frecuencia\n",
    "    for word, freq in vocab.items():\n",
    "        # Separa las palabras por letra\n",
    "        symbols = word.split()\n",
    "        # Esto se lee \"Recorre el largo de\"\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "# get_stats(get_vocab('miniCorpus.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab(pair, v_in)\n",
    "Función que recibe una pareja de palabras (pair) y un vocabulario (v_in) y devuelve otro vocabulario nuevo (v_out) con las nuevas parejas de palabras concatenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    #Crea un diccionario vacío\n",
    "    v_out = {}\n",
    "    # Quita los caracteres especiales\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Crea expresión regular para buscar un bigrama que esté rodeado por caracteres \n",
    "    # no blancos y que no esté precedido ni seguido por otras palabras. \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # Reemplaza la cadena bigram con la pareja concatenada y lo agrega en v_out\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    # Regresa el nuevo vocabulario\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minicorpus a usar :\n",
    "# https://drive.google.com/file/d/17h_rLrWL2xg3jD0U1CCseeaAd6t17yc0/view?usp=share_linket \n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 22, ',</w>': 14, 'J': 1, 'u': 11, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 18, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 52\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# En este bloque de código se muestran un diccionario de todas las palabras en el vocabulario.\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('o', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 115, 'J': 1, 'u': 18, 'a': 56, 'n': 39, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e': 90, 'A': 1, 'r': 34, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 41\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('e', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 91, 'J': 1, 'u': 18, 'a': 56, 'n': 39, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 66, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 42\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('a', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 72, 'J': 1, 'u': 18, 'a': 37, 'n': 39, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 66, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 43\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: ('e', 'n')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',': 14, '</w>': 72, 'J': 1, 'u': 18, 'a': 37, 'n': 22, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 49, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 44\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: (',', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 58, 'G': 1, 'l': 36, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 49, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 44\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('l', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 45, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'e': 49, 's': 37, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'l</w>': 13, 'R': 1, 'y': 12, 't': 23, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 45\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('e', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 45, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'e': 37, 'l</w>': 13, 'R': 1, 'y': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 46\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('y', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 44, 'e</w>': 24, 'A': 1, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'e': 37, 'l</w>': 13, 'R': 1, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 46\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('d', 'e')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 23, 'o</w>': 24, 'd': 35, 'e</w>': 24, 'A': 1, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 47\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('d', 'e</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 23, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 27, 'r': 34, 'es': 12, 'c': 22, 'i': 42, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 48\n",
      "==========\n",
      "Iter: 10\n",
      "Best pair: ('l', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 27, 'r': 34, 'es': 12, 'c': 22, 'i': 34, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 49\n",
      "==========\n",
      "Iter: 11\n",
      "Best pair: ('d', 'i')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 37, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 34, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 50\n",
      "==========\n",
      "Iter: 12\n",
      "Best pair: ('a', 'r')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 29, ',</w>': 14, 'J': 1, 'u': 18, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 25, 'ñ': 3, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 51\n",
      "==========\n",
      "Iter: 13\n",
      "Best pair: ('o', 's')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 22, ',</w>': 14, 'J': 1, 'u': 18, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 18, 'ñ': 3, 'os': 7, 'q': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 52\n",
      "==========\n",
      "Iter: 14\n",
      "Best pair: ('q', 'u')\n",
      "Tokens: defaultdict(<class 'int'>, {'Y': 2, 'o': 22, ',</w>': 14, 'J': 1, 'u': 11, 'a': 30, 'n': 22, '</w>': 33, 'G': 1, 'l': 15, 'o</w>': 24, 'de</w>': 8, 'A': 1, 'd': 19, 'r': 27, 'es': 12, 'c': 22, 'i': 26, 'b': 7, 'C': 3, 'á': 1, 'm': 11, 'ar': 7, 'a</w>': 19, 'de': 9, 'l</w>': 13, 'R': 1, 'e': 28, 'y</w>': 12, 't': 23, 's': 18, 'ñ': 3, 'os': 7, 'qu': 7, 'e</w>': 16, 'en': 17, 'j': 1, 'f': 2, 'h': 9, 'v': 10, 'p': 17, 'é': 1, 'li': 8, 'E': 1, 'g': 6, 'M': 2, 'S': 1, 'di': 8, 'í': 3, ';': 2, '.': 2, 'V': 1})\n",
      "Number of tokens: 52\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_vocab(filename):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "\n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    string_tokens = []\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "\n",
    "vocab = get_vocab('miniCorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan los tokens antes de aplicarles el algoritmo de Byte Pair Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 10000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando la tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_given_known = 'mountains</w>'\n",
    "word_given_unknown = 'Ilikeeatingapples!</w>'\n",
    "\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra conocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_given = word_given_known \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una palabra desconocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_given = word_given_unknown \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "42fe2521991ce668210cff6e1c62959905284a5dc1077ee025bbb92f9d2c0e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
